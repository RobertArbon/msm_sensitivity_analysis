{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe2541a-9868-47b6-bb48-022f98d22b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import matplotlib as mpl\n",
    "import pymc as pm\n",
    "import scipy as sp\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler, minmax_scale, scale, robust_scale\n",
    "import patsy as pt\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cf239ec8-c947-42b9-a1b6-1c1667df9530",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gamma(alpha, beta):\n",
    "    def g(x):\n",
    "        return pm.Gamma(x, alpha=alpha, beta=beta)\n",
    "    return g\n",
    "\n",
    "def hcauchy(beta):\n",
    "    def g(x):\n",
    "        return pm.HalfCauchy(x, beta=beta)\n",
    "    return g\n",
    "\n",
    "\n",
    "def fit_gp(y, X, l_prior, eta_prior, sigma_prior, nu_prior=None, kernel_type='M52', \n",
    "           bayes_kws=dict(draws=1000, tune=1000, chains=2, cores=1), prop_Xu=None, \n",
    "           model_type='marginal', noise_model='normal', n_ppc=0):\n",
    "    \"\"\"\n",
    "    function to return a pymc3 model\n",
    "    y : dependent variable\n",
    "    X : independent variables\n",
    "    prop_Xu : number of inducing varibles to use. If None, use full marginal likelihood. If not none, use FTIC. \n",
    "    bayes_kw : kws for pm.sample\n",
    "    X, y are dataframes. We'll use the column names. \n",
    "    \"\"\"\n",
    "    kernel_type = kernel_type.lower()\n",
    "    with pm.Model() as model:\n",
    "        # Covert arrays\n",
    "        X_a = X.values\n",
    "        y_a = y.values.flatten()\n",
    "        X_cols = list(X.columns)\n",
    "\n",
    "        \n",
    "        # Kernels\n",
    "        # 3 way interaction\n",
    "        eta = eta_prior('eta')\n",
    "        cov = eta**2\n",
    "        for i in range(X_a.shape[1]):\n",
    "            var_lab = 'l_'+X_cols[i]\n",
    "            if kernel_type=='rbf':\n",
    "                cov = cov*pm.gp.cov.ExpQuad(X_a.shape[1], ls=l_prior(var_lab), active_dims=[i])\n",
    "            if kernel_type=='exponential':\n",
    "                cov = cov*pm.gp.cov.Exponential(X_a.shape[1], ls=l_prior(var_lab), active_dims=[i])\n",
    "            if kernel_type=='m52':\n",
    "                cov = cov*pm.gp.cov.Matern52(X_a.shape[1], ls=l_prior(var_lab), active_dims=[i])\n",
    "            if kernel_type=='m32':\n",
    "                cov = cov*pm.gp.cov.Matern32(X_a.shape[1], ls=l_prior(var_lab), active_dims=[i])\n",
    "\n",
    "        # Covariance model\n",
    "        cov_tot = cov \n",
    "        \n",
    "        sigma_n =sigma_prior('sigma_n')\n",
    "\n",
    "        # Model\n",
    "        if model_type=='latent': \n",
    "            gp = pm.gp.Latent(cov_func=cov_tot)\n",
    "            f = gp.prior(\"f\", X=X_a)\n",
    "            if noise_model == 'normal':\n",
    "                y_ = pm.Normal(\"y_\", mu=f, sigma=sigma_n, observed=y_a)\n",
    "            elif noise_model == 'TP': \n",
    "                nu = nu_prior('nu')\n",
    "                y_ = pm.StudentT(\"y\", mu=f, lam=1.0 / sigma_n, nu=nu, observed=y_a)\n",
    "            else:\n",
    "                raise ValueError('must specify noise for latent model')\n",
    "                \n",
    "        elif model_type=='TP':\n",
    "            nu1 = nu_prior('nu1')\n",
    "            tp = pm.gp.TP(cov_func=cov_tot, nu=nu1)\n",
    "            f = tp.prior(\"f\", X=X_a)\n",
    "            \n",
    "            if noise_model == 'normal':\n",
    "                y_ = pm.Normal(\"y_\", mu=f, sigma=sigma_n, observed=y_a)\n",
    "            elif noise_model == 'TP': \n",
    "                nu2 = nu_prior('nu2')\n",
    "                y_ = pm.StudentT(\"y\", mu=f, lam=1.0 / sigma_n, nu=nu2, observed=y_a)\n",
    "            else:\n",
    "                raise ValueError('must specify noise for TP model')            \n",
    "            \n",
    "            gp = tp\n",
    "            \n",
    "        elif model_type=='marginal':\n",
    "            # Noise model\n",
    "\n",
    "            if not (prop_Xu is None):\n",
    "                # Inducing variables\n",
    "                num_Xu = int(X_a.shape[0]*prop_Xu)\n",
    "                Xu = pm.gp.util.kmeans_inducing_points(num_Xu, X_a)\n",
    "                gp = pm.gp.MarginalSparse(cov_func=cov_tot, approx=\"FITC\")\n",
    "                y_ = gp.marginal_likelihood('y_', X=X_a, y=y_a, Xu=Xu, noise=sigma_n)\n",
    "            else:\n",
    "                gp = pm.gp.Marginal(cov_func=cov_tot)\n",
    "                y_ = gp.marginal_likelihood('y_', X=X_a, y=y_a, noise=sigma_n)\n",
    "            \n",
    "        if n_ppc > 0: \n",
    "            result = pm.sample_prior_predictive(samples=n_ppc)\n",
    "        else:\n",
    "            if not (bayes_kws is None):\n",
    "                trace = pm.sample(**bayes_kws)\n",
    "                result = trace\n",
    "            else:\n",
    "                start_val = 0.01\n",
    "                x0 = {'l_dim': start_val, 'l_lag': start_val, 'l_steep': start_val, \n",
    "                      'l_cent': start_val, \n",
    "                      'l_states': start_val, \n",
    "                     'l_scheme[T.closest-heavy]':start_val, \n",
    "                     'eta': 1, 'sigma_n': 1}\n",
    "                \n",
    "#                 # options={'disp': None, 'maxcor': 10, \n",
    "#                 #          'ftol': 2.220446049250313e-09, 'gtol': 1e-05, \n",
    "#                 #          'eps': 1e-08, 'maxfun': 15000, 'maxiter': 15000, \n",
    "#                 #          'iprint': - 1, 'maxls': 20, \n",
    "#                 #          'finite_diff_rel_step': None}\n",
    "#                 options={'disp': None, 'maxcor': 10, \n",
    "#                          'ftol': 2.220446049250313e-12, 'gtol': 1e-12, \n",
    "#                          'eps': 1e-12, 'maxfun': 15000, 'maxiter': 15000, \n",
    "#                          'iprint': - 1, 'maxls': 20, \n",
    "#                          'finite_diff_rel_step': None}\n",
    "                # mp = pm.find_MAP(start = x0, options=options)\n",
    "                mp = pm.find_MAP(start = x0)\n",
    "\n",
    "                result = mp\n",
    "    \n",
    "    return gp, result, model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scale_df(df, formula, feature_range=None):\n",
    "    new_df = []\n",
    "    df = df.copy(deep=True)\n",
    "    \n",
    "    dv = formula.split('~')[0].strip()\n",
    "    x = df[dv].values.reshape(-1, 1)\n",
    "    print(f'dv is {dv}')\n",
    "    if dv == 'ts': \n",
    "        x = robust_scale(np.log(x))\n",
    "        dv_range = (x.min(), x.max())\n",
    "    elif dv == 'vamp2_eq':\n",
    "        x = robust_scale(x)\n",
    "        dv_range = (x.min(), x.max())\n",
    "    else: \n",
    "        raise ValueError('no dep var recognized')\n",
    "    new_df.append(pd.Series(x.flatten(), name=dv))\n",
    "    \n",
    "    print(f'dv range is {dv_range}')    \n",
    "    if feature_range is None: \n",
    "        feature_range = dv_range\n",
    "    \n",
    "    df.drop(labels=dv, axis=1, inplace=True)\n",
    "    \n",
    "    for col in df.columns: \n",
    "        x = df[col].values.reshape(-1, 1)\n",
    "        if col == 'hp_ix': \n",
    "            pass\n",
    "        elif col in ['dim', 'lag', 'steep', 'cent', 'states']: \n",
    "            x = minmax_scale(x, feature_range=feature_range)\n",
    "        else: \n",
    "            pass\n",
    "        new_df.append(pd.Series(x.flatten(), name=col))\n",
    "            \n",
    "    new_df = pd.concat(new_df, axis=1)\n",
    "    return new_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9562b0de-c76e-41f0-97bf-0fe27b2eaf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formula(dv, trans, feat): \n",
    "    formula = f\"{dv} ~  dim + lag + states\"\n",
    "    if feat == 'distances': \n",
    "        formula += ' + scheme'\n",
    "        if (trans == 'logistic'): \n",
    "            formula += \" + steep + cent\"\n",
    "    print(formula)\n",
    "    return formula\n",
    "\n",
    "\n",
    "def get_dataframes(data_s, feat, trans, formula): \n",
    "    X = data_s.query(f\"(feat == '{feat}')\")\n",
    "    if trans is not None: \n",
    "        X = X.query(f\"trans == '{trans}'\")\n",
    "    ydf, Xdf = pt.dmatrices(formula, data=X, return_type='dataframe', NA_action='raise')\n",
    "    Xdf.drop(labels=['Intercept'], axis=1, inplace=True)\n",
    "    return ydf, Xdf\n",
    "\n",
    "def subset_data(ydf, Xdf, hp_ix=None):\n",
    "    if hp_ix is not None: \n",
    "        ix = data.hp_ix < hp_ix\n",
    "        ydf = ydf.loc[ix, :]\n",
    "        Xdf = Xdf.loc[ix, :] \n",
    "        \n",
    "    return ydf, Xdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7e4dbe8f-c6b8-4294-8e3c-cb009208184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein = '1fme'\n",
    "lag=41\n",
    "proc=2\n",
    "\n",
    "# pre-processing params\n",
    "data_cols = ['median', 'tica__dim', 'tica__lag', 'cluster__k', 'feature__value', 'distances__scheme', 'distances__transform', \n",
    "             'distances__steepness', 'distances__centre'\n",
    "]\n",
    "var_names_short = ['ts', 'dim', 'lag', 'states', 'feat', 'scheme', 'trans', 'steep', 'cent']\n",
    "name_dict = dict(zip(data_cols, var_names_short))\n",
    "scaling = dict(dim=[1, 20], lag=[1, 100],states=[10, 500], steep=[0, 50], cent=[0, 1.5])\n",
    "\n",
    "# Bayesian kws\n",
    "bayes_kws = dict(draws=1000, tune=1000, chains=4, cores=4, target_accept=0.90)\n",
    "\n",
    "# Load data\n",
    "summary_path = f'../{protein}/summary.h5'\n",
    "hp_paths = ['../../experiments/hpsample.h5', '../../experiments/new_hpsample.h5']\n",
    "hps = []\n",
    "for hp_path in hp_paths:\n",
    "    hp = pd.read_hdf(hp_path)\n",
    "    hp.reset_index(inplace=True)\n",
    "    hps.append(hp)\n",
    "hps = pd.concat(hps)\n",
    "\n",
    "timescales = pd.read_hdf(summary_path, key='timescales')\n",
    "vamps = pd.read_hdf(summary_path, key='vamps')\n",
    "timescales.reset_index(inplace=True)\n",
    "vamps.reset_index(inplace=True)\n",
    "\n",
    "# Create main data DF\n",
    "data = timescales.query(f\"process=={proc}\").query(f'lag=={lag}')\n",
    "data = data.merge(hps, on=['hp_ix'], how='left')\n",
    "data = data.loc[:, data_cols+['hp_ix']]\n",
    "data.rename(mapper=name_dict, axis=1, inplace=True)\n",
    "\n",
    "# Add GMRQ from timescale: \n",
    "data['vamp2_eq'] = 1+np.exp(-lag/data['ts'])**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52255e6-b426-48c9-b90c-f2d48b2143d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "478ef5a8-e90f-4e90-a262-30d75545e709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "params = [['dihedrals', None], ['distances', 'linear'], ['distances', 'logistic']]\n",
    "kernels = ['exponential', 'rbf', 'm32', 'm52']\n",
    "dv = 'vamp2_eq'\n",
    "\n",
    "\n",
    "out_dir = Path(f'../{protein}').joinpath('sensitivity', dv)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for kernel in kernels: \n",
    "    print(kernel)\n",
    "    for feat, trans in params:\n",
    "        print(feat, trans)\n",
    "        \n",
    "        out_path = out_dir.joinpath(f\"{feat}_{trans}_{kernel}.pkl\")\n",
    "\n",
    "        formula = get_formula(dv, trans, feat)\n",
    "        data_s = scale_df(data, formula)\n",
    "        ydf, Xdf = get_dataframes(data_s, feat, trans, formula)\n",
    "        ydf, Xdf = subset_data(ydf, Xdf, hp_ix=329)\n",
    "\n",
    "        gp, mp, model = fit_gp(y=ydf, X=Xdf,  \n",
    "                                l_prior=l_prior, eta_prior=eta_prior, sigma_prior=sigma_prior,\n",
    "                                kernel_type=kernel, \n",
    "                                prop_Xu=None,  \n",
    "                                bayes_kws=None, model_type='marginal', \n",
    "                                noise_model=None, nu_prior=None, \n",
    "                                n_ppc=0)  \n",
    "        with model: \n",
    "            y_pred, var = gp.predict(Xdf.values,point=trace, diag=True)\n",
    "\n",
    "        rmse = np.sqrt(np.sum((ydf.values.flatten()-y_pred)**2))\n",
    "\n",
    "        results = {'trace': trace, 'data': pd.concat([ydf,Xdf], axis=1), 'formula': formula, 'lag': lag, 'proc': proc}\n",
    "\n",
    "        pickle.dump(obj=results, file=out_path.open('wb'))\n",
    "\n",
    "        with sns.plotting_context('paper'):\n",
    "            plt.scatter(ydf.values, y_pred)\n",
    "            plt.title(f\"rmse: {rmse:4.2f}\")\n",
    "            lim = np.max([np.abs(np.min(ydf.values)), np.max(ydf.values)])*1.1\n",
    "            plt.ylim(-lim, lim)\n",
    "            plt.xlim(-lim, lim)\n",
    "            plt.plot([-lim, lim], [-lim, lim])\n",
    "            plt.xlabel('observed values')\n",
    "            plt.ylabel('predicted values')\n",
    "\n",
    "            plt.savefig(out_path.with_suffix('.pdf'), bbox_inches='tight')\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd27154c-6579-4cb6-8bd7-59b2c023b6b6",
   "metadata": {},
   "source": [
    "Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "63780327-afa9-4ba6-81cf-35770d094576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.linspace(0, 5, 100)\n",
    "# fig, ax = plt.subplots(2)\n",
    "\n",
    "# pdf1 = pm.Gamma.dist(1, 10)\n",
    "# ax[0].plot(x, np.exp(pm.logp(pdf1, x).eval()))\n",
    "# pdf2 = pm.HalfCauchy.dist(2)\n",
    "# ax[1].plot(x, np.exp(pm.logp(pdf2, x).eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8e36fc8a-f79f-451a-b6a4-6bd58d9dd138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_prior = gamma(2, 0.5)\n",
    "# eta_prior = hcauchy(2)\n",
    "# sigma_prior = hcauchy(2)\n",
    "# kernel='exponential'\n",
    "# trans = 'logistic'\n",
    "# feat = 'distances'\n",
    "# dv = 'vamp2_eq'\n",
    "\n",
    "# formula = get_formula(dv, trans, feat)\n",
    "# data_s = scale_df(data, formula)\n",
    "# ydf, Xdf = get_dataframes(data_s, feat, trans, formula)\n",
    "# ydf, Xdf = subset_data(ydf, Xdf, hp_ix=329)\n",
    "\n",
    "# gp, mp, model = fit_gp(y=ydf, X=Xdf,  \n",
    "#                         l_prior=l_prior, eta_prior=eta_prior, sigma_prior=sigma_prior,\n",
    "#                         kernel_type=kernel, \n",
    "#                         prop_Xu=None,  \n",
    "#                         bayes_kws=None, model_type='marginal', \n",
    "#                         noise_model=None, nu_prior=None, \n",
    "#                         n_ppc=0)  \n",
    "# # Prediction\n",
    "# with model: \n",
    "#     y_pred, var = gp.predict(Xdf.values,point=mp, diag=True)\n",
    "\n",
    "#     rmse = np.sqrt(np.sum((ydf.values.flatten()-y_pred)**2))\n",
    "\n",
    "# # Plot result\n",
    "# with sns.plotting_context('paper'):\n",
    "#     plt.scatter(ydf.values, y_pred)\n",
    "#     plt.title(f\"rmse: {rmse:4.2f}\")\n",
    "#     lim = np.max([np.abs(np.min(ydf.values)), np.max(ydf.values)])*1.1\n",
    "#     plt.ylim(-lim, lim)\n",
    "#     plt.xlim(-lim, lim)\n",
    "#     # plt.ylim(-9, 9)\n",
    "#     # plt.xlim(-9, 9)\n",
    "#     plt.plot([-lim, lim], [-lim, lim])\n",
    "#     plt.xlabel('observed values')\n",
    "#     plt.ylabel('predicted values')\n",
    "# print(f\"{1/mp['l_cent']:4.2f}, {1/mp['l_steep']:4.2f}, {1/mp['l_dim']:4.2f}, {1/mp['l_lag']:4.2f}, {1/mp['l_states']:4.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fef9b290-61ef-4343-befd-3e75b87db6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = plt.hist(np.array(ppc.prior_predictive['y_']).flatten(), bins=1000)\n",
    "\n",
    "# plt.ylim(0, 100)\n",
    "    \n",
    "# print(np.mean(np.abs(np.array(ppc.prior_predictive['y_']).flatten())>10))\n",
    "# # plt.xlim(-10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4752be12-78c6-46d3-9009-b0de4b64fce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
